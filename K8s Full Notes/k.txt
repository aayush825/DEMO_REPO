https://chatgpt.com/share/687f5e55-69c8-8013-9b21-49870106d9b1

COMPONENTS OF KUBERNETES MASTER NODE ----

API SERVER -- API server is the component of the master node, which will directly interact with the Kubernetes admin Whatever the command, whatever the API you are hitting on the K8s, they directly will be received by the Kubernetes API server.
So Kubernetes API server is the component of the master node, which will accept the command from the developer or the outer world, and they will issue the response to the developer or the outer world.

ETCD -- which is a distributed key value pair based database. That is something you can assume a backend service for the Kubernetes, whatever the state of your Kubernetes cluster, that is state must be saved in the seed. So it is a database for your Kubernetes cluster, which will store your Kubernetes cluster state.

CONTROL MANAGER --- Control manager will basically control the complete control plane of your communities.
A machine (or VM) where application workloads actually run, An agent that runs on every Worker Node.
That includes the node controller, replication controller, endpoint controller, and the token controller.
So all these tasks are basically being managed by the control manager in your Kubernetes cluster.

SCEDULER --- Another and last component of the Kubernetes cluster master node is the scheduler. Scheduler directly communicate with the API server and it will decide that what resource need to be scheduled on what worker node. scheduler is the component which will decide that this is the resource and this need to be scheduled on this particular worker node. So scheduler is something which will manage the resource deployment on the worker nodes in communities cluster.

Kube ApiServer- the only interaction point to interact with the human cluster.

---------------------------------------------------------------------------------------------------------------------------------------------------------

ETCD - That is a database in your Kubernetes cluster that is a distributed key value store for the cluster state.
Example etcd- >> How many resources are running in your cluster?
What is the status of these resources?
What is the configuration of these resources?
At which particular order the resources are being deployed?
If anything happened to any resources, any time.
What is the what is the health check of these sources?
What is the liveness probe or readiness probe of these resources?
Every information about your cluster and cluster estate is stored in the etcd, and that is a distributed key value store.
-----------------------------------------------------------------------------------------------------------------------------

Kube Scheduler - regulate task on slave nodes
kube controller  -- manage Kubernetes jobs, manage all probes and auto jobs

------------------------------------------
------------------------------------------

kubelet checks whether the container are running or not, it accept the command from master node, it connects the worker and the master node

Pods - Group/single multiple container with shared storage/network, container execute within the pods

service proxy - helps you in accessing pods(it container container) with help of api server, it ensures application is accessible to outer parties

kubectl is kube controller, it connects with api server and can handle all the cluster

--------------------------------------------------------------------------------------------------------------------------------------------

Kubenets is working on REST Api
so whenever we are creating the YAML file for your Kubernetes object, you need to define this API version.

API version is a combination of the API group / API version.
Metadata is something which will define the associated information with your object.
Labels : Labels are key-value pairs that are attached to K8s objects, such as pods.

FOUR IMP THINGS TO CREATE A SERVICE IN YML FILE:

"apiversion, kind, metadata, spec"

Lecture 29: imp pdf starts from here

---

basically if you forgot cyntax duuring exam, you can do this:

HOW TO CREATE A MANIFEST FILE FOR POD USING CLI?

kubectl run nginx-webapp --image=nginx:1.20 --dry-run=client || this will basically not create a pod

now if you write below command you will get the manifest file

kubectl run nginx-webapp --image=nginx:1.20 --dry-run=client -o yaml

\*if you dont know thee syntax of port then you can write below command

kubectl run nginx-webapp --image=nginx:1.20 --port=8080 --dry-run=client -o yaml

---

Lecture 34

ğŸ§± In Docker, there are 2 instructions related to starting a container:

| Dockerfile Instruction | Purpose                                                                            |
| ---------------------- | ---------------------------------------------------------------------------------- |
| `ENTRYPOINT`           | Specifies the main command to run                                                  |
| `CMD`                  | Provides default arguments to the ENTRYPOINT (or command if ENTRYPOINT isn't used) |


ğŸ‘‰ So in Docker:

ENTRYPOINT is like the fixed command (like the tool or script).
CMD is like the default arguments passed to the ENTRYPOINT.

âœ… Example 2: Using ENTRYPOINT and CMD

FROM ubuntu
ENTRYPOINT \["echo"]
CMD \["Hello from CMD"]

docker run my-entry-cmd "Hi from user"

ğŸ“¦ Output:
Hi from user

âœ… ENTRYPOINT can be overridden using --entrypoint in Docker.

for example:

ENTRYPOINT = python
CMD = script.py

everytime cmd will change not entrypoint as cmd consist of file name script1.py run.py etc

---

ğŸ”¹ PART 2: Kubernetes â€” command vs args

| Concept              | Docker       | Kubernetes |
| -------------------- | ------------ | ---------- |
| Main command         | `ENTRYPOINT` | `command`  |
| Arguments to command | `CMD`        | `args`     |


Example:

apiVersion: v1
kind: Pod
metadata:
  name: sleep-pod
spec:
  containers:
    - name: sleeper
      image: ubuntu
      command: ["sleep"]
      args: ["30"]


ğŸ§ª You can even override both ENTRYPOINT and CMD:

command: \["/bin/sh"]
args: \["-c", "echo Hello from K8s \&\& sleep 5"]

-----------------------------------------------------------------------------------------------------------------------------------------------

Multi-Container Pod Design Patterns

\*Sidecar pattern- A helper container runs alongside the main application to extend its functionality.
Example: Add a logging agent, reverse proxy, or monitoring agent next to the main container

\*Adapter Pattern - A container is added to modify or transform the output of the main application before sending it to another system.

\*Ambassador Pattern - A proxy container manages communication between the application and external services.

There is Init Container also - it works same like constructor like in the starting of pod and the ending of pod

eg- my app will not initialize until the db intialixze , then we can put entry check

---------------------------------------------------------------------------------------------------------------------------------------------------

Stateless application are those application when restarted will start from fresh.

stateful - those app which needs previous states

------------------------------------------------------------------------------------------------------------------------------------------

ğŸ”´ Why You Should Avoid Multi-Container Pods

All containers in a pod share the same lifecycle â€” they are started, stopped, scheduled, and restarted together.
If one crashes or is updated, the entire pod restarts, affecting other containers.
This breaks microservices principles where each service should be independent.
If you need to scale one container (like the frontend), but not the other (like a helper), you canâ€™t do it separately.
Youâ€™ll have to replicate the whole pod, which is wasteful and inefficient.
Harder to find which container is misbehaving.
Troubleshooting multiple containers inside one pod can become messy, especially if they generate similar logs.


ğŸ§  Final Thought
| Design Principle | Why You Avoid Multi-Container Pods                       |
| ---------------- | -------------------------------------------------------- |
| Microservices    | Each component should run, scale, and fail independently |
| Observability    | Easier to log, monitor, and debug one container per pod  |
| Security         | Isolation is stronger with single-container pods         |
| Deployment       | Independent CI/CD pipelines for each component           |


-------------------------------------------------------------------------------------------------------------------------------------------------

ğŸ”¹ What is a Job in Kubernetes?

Job : Job is used to Execute Ad-Hoc Tasks in Kubernetes.

â—‹ Job will execute until task completed.
â—‹ Job will create a Pod and mark it completed once complete.
â—‹ Job wonâ€™t delete the Pod on Job Completion

-> When a Job completes, the Pod it created still exists â€” it doesn't get deleted immediately.

You can still view the Pod using: kubectl get pods
â—‹ User can extract the logs from Pod Logs as job complete/fail.
â—‹ On deletion of Job, associated Pod also delete.

->If you delete the Job resource, Kubernetes will also delete the Pod created by it.

example: kubectl delete job simple-job (after this if you do kubectl get pods : you will not see the pod)

ğŸ§  Optional Bonus: TTL for Jobs ALSO CALLED Cleanup Policy
If you donâ€™t want old Job pods hanging around, add this:

spec:
ttlSecondsAfterFinished: 60 # Delete Pod 60 seconds after completion

Then the Pod will auto-delete after 1 minute of finishing.

ğŸ“¦EXAMPLE OF YML FILE OF JOB:

apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  template:
    spec:
      containers:
        - name: hello
          image: busybox
          command: ["echo", "Hello from the job!"]
      restartPolicy: Never


ğŸ”¹ 1. What is Restart Policy in Kubernetes?

The restart policy tells Kubernetes what to do if a container in a pod exits or crashes.

| Restart Policy     | Description                                                     | Common Use Case              |
| ------------------ | --------------------------------------------------------------- | ---------------------------- |
| `Always` (default) | Always restart the container if it exits, **even successfully** | Deployments                  |
| `OnFailure`        | Restart **only if** container exits with error (non-zero code)  | Jobs                         |
| `Never`            | Never restart the container, even if it crashes                 | Debug Pods or one-time tasks |


ğŸ”¹ What is a CronJob in Kubernetes?

A CronJob is like a Job, but it runs on a schedule, similar to a cron task in Linux.
It creates Jobs on the schedule you define using cron syntax.

âœ… Example Use Case:

Daily backup at midnight
Hourly log clean-up
Weekly report generation
Ping a URL every 5 minutes

ğŸ§ª Example YAML for a CronJob

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-cron
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: hello
              image: busybox
              command: ["echo", "Hello from the cronjob!"]
          restartPolicy: Never


Each job runs echo "Hello from the cronjob!" and exits.

ğŸ” What Happens Behind the Scenes?

A Job creates a Pod to run your task.
A CronJob creates a Job on schedule â†’ which then creates a Pod.

ğŸ”§ Important Job.spec Fields Explained

completions : Defines how many successful Pods need to run to complete the Job.
parallelism: How many Pods can run at the same time.
ttlSecondsAfterFinished : Auto-delete the Job and Pods after completion (success or failure).
activeDeadlineSeconds : Maximum total runtime (in seconds) for all Pods under this Job.
activeDeadlineSeconds: 60 (â¡ï¸ If Job hasnâ€™t completed in 60 seconds, it will be stopped (status = Failed).)

EXAMPLE:

apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  completions: 5
  parallelism: 2
  ttlSecondsAfterFinished: 120
  activeDeadlineSeconds: 60
  template:
    spec:
      containers:
        - name: worker
          image: busybox
          command: ["sh", "-c", "echo Hello && sleep 10"]
      restartPolicy: OnFailure


ğŸ“Œ Additional Options in Jobs & CronJobs

| Field                        | Purpose                                      |
| ---------------------------- | -------------------------------------------- |
| `completions`                | Total successful Pods required               |
| `parallelism`                | How many Pods to run at once                 |
| `backoffLimit`               | Retry attempts if the job fails              |
| `startingDeadlineSeconds`    | Deadline to start missed jobs (CronJob only) |
| `successfulJobsHistoryLimit` | How many successful job histories to keep    |
| `failedJobsHistoryLimit`     | Same, but for failed jobs                    |


âœ… 1. completions
Defines how many successful Pods need to run to complete the Job.
By default: completions: 1
Once N Pods finish successfully, the job is marked as complete.

âœ… 4. activeDeadlineSeconds
Maximum total runtime (in seconds) for all Pods under this Job.
If the Job does not complete within this time, it's terminated
Useful for timeout control


-------------------------------------------------------------------------------------------------------------------------------

Volume Types In K8s:

Temp Data:
a. Emptyir -> Data Will be deleted as soon as pod deleted
b. Hostpath -> Data will retain on Host Directory even Pod Deleted.

Persistent Data: whenever you need to store that data, right, and your application will restart or reboot, that will read the state of previously stored application or that will start from the data which is basically created previous application. That data is called the persistent data, which data is going to persist on the disk and that will not remove the application that is called the persistent data, given it is provide a variety of the persistent data.

a. gcePersistentDisk - you can create your persistent data on the Google cloud disk using the GC persistent disk
b. azureDisk
c. awsElasticBlockStorage

Netwrk Data Storage:
a. NFS
b. ISCSI
c. glusterFS

Configuration Data Storage:
a. ConfigMaps
b. Secrets
c. downwardAPI

For Persistent data:

ğŸ§± Structure of Configuration:

Pod
â””â”€â”€ uses PVC
â””â”€â”€ binds to PV
â””â”€â”€ backed by cloud disk (like azureDisk, gcePersistentDisk, awsEBS) || it means basically the real storage is created in cloud

âœ… Full Working Example: azureDisk

ğŸ”¹ Step 1: Define a Persistent Volume (PV)

apiVersion: v1
kind: PersistentVolume
metadata:
  name: azure-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  azureDisk:
    diskName: my-managed-disk
    diskURI: /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.Compute/disks/my-managed-disk
    kind: Managed
    cachingMode: ReadOnly
    fsType: ext4
  persistentVolumeReclaimPolicy: Retain


ğŸ”¹ Step 2: Define a Persistent Volume Claim (PVC)

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: azure-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi


ğŸ”¹ Step 3: Use the PVC in a Pod

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: mycontainer
      image: nginx
      volumeMounts:
        - mountPath: "/mnt/data"
          name: mydata
  volumes:
    - name: mydata
      persistentVolumeClaim:
        claimName: azure-pvc



ğŸ§  Summary Table

| Field                                                      | Purpose                                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------- |
| `PersistentVolume (PV)`                                    | Describes a piece of external/durable storage                                   |
| `PersistentVolumeClaim (PVC)`                              | Requests a PV                                                                   |
| `azureDisk` / `gcePersistentDisk` / `awsElasticBlockStore` | Cloud provider-specific volume type                                             |
| `volumeMounts`                                             | Mounts the PVC inside a container                                               |
| `accessModes`                                              | E.g., `ReadWriteOnce`, `ReadOnlyMany`, etc.                                     |
| `persistentVolumeReclaimPolicy`                            | What to do with the volume after PVC is deleted (`Retain`, `Recycle`, `Delete`) |


ğŸ“Œ This pod mounts the persistent disk at /mnt/data. Data written there survives even if the pod is deleted or restarted.

ğŸ”— So, How Are They Matched?

PVC and PV are matched based on:
âœ… Storage size (resources.requests.storage)
âœ… Access mode (ReadWriteOnce, ReadOnlyMany, etc.)
âœ… Labels + Selectors (optional) â€” if you want to bind to a specific PV

here it matches on basis of below:

spec:
accessModes:
- ReadWriteOnce
capacity:
storage: 5Gi

ğŸ§ª But if you want to bind to a specific PV, you can use label selectors:
example: â€œBind me only to a PV that has label type=fast-diskâ€

â“ If a PV is created with 5GiB, and I create a PVC asking for 4GiB â€” will it be bound?
ğŸ‘‰ âœ… Yes, it will be bound!

ğŸ“Œ Reason:
Kubernetes allows a PV that has equal or more capacity than requested by the PVC.

ğŸ” Matching Rule (simplified):

| PVC asks for | PV has | Match?                   |
| ------------ | ------ | ------------------------ |
| 4Gi          | 5Gi    | âœ… Yes                    |
| 5Gi          | 5Gi    | âœ… Yes                    |
| 6Gi          | 5Gi    | âŒ No  (not enough space) |


ğŸ§  BUT â€” Important Note:

Once bound:

Even though the PV has 5GiB, the PVC only uses 4GiB.
The remaining 1GiB is not used by others â€” it is reserved with the PV.
One PV = one PVC only (unless ReadOnlyMany access is used).

â¡ï¸ Kubernetes binds the whole PV to the PVC â€” itâ€™s fully reserved, even though you asked for less.
A PV can be bound to only one PVC (unless RWX or ROX)
ğŸ§© Visual Diagram:

             [  PV: 5Gi, RWO  ]
                    â¬‡ï¸
          PVC: 4Gi, RWO â†’ Bound âœ…
                    â¬‡ï¸
               Pod uses it

             âŒ No other PVC can use remaining 1GiB!


ğŸ§  Summary Table

| Field                                                      | Purpose                                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------- |
| `PersistentVolume (PV)`                                    | Describes a piece of external/durable storage                                   |
| `PersistentVolumeClaim (PVC)`                              | Requests a PV                                                                   |
| `azureDisk` / `gcePersistentDisk` / `awsElasticBlockStore` | Cloud provider-specific volume type                                             |
| `volumeMounts`                                             | Mounts the PVC inside a container                                               |
| `accessModes`                                              | E.g., `ReadWriteOnce`, `ReadOnlyMany`, etc.                                     |
| `persistentVolumeReclaimPolicy`                            | What to do with the volume after PVC is deleted (`Retain`, `Recycle`, `Delete`) |



ğŸ”· What are Access Modes?

Access modes define how many Pods can access the volume and how (read/write or read-only).
They are specified in both:
PV (PersistentVolume.spec.accessModes)
PVC (PersistentVolumeClaim.spec.accessModes)

â¡ï¸ Kubernetes will only bind a PV to a PVC if access modes match.

ğŸ“˜ Types of Access Modes

| Access Mode           | Description                                                          |
| --------------------- | -------------------------------------------------------------------- |
| `ReadWriteOnce` (RWO) | Volume can be mounted as **read-write by only one node** at a time   |
| `ReadOnlyMany` (ROX)  | Volume can be mounted as **read-only by many nodes**                 |
| `ReadWriteMany` (RWX) | Volume can be mounted as **read-write by many nodes** simultaneously |


âœ… 1. ReadWriteOnce (RWO) -> Only one Pod on one node can write and read from the volume

ğŸ“¦ Example Use Case:
A single-instance MySQL database Pod
You don't want multiple apps writing to the DB at the same time

âœ… 2. ReadOnlyMany (ROX) -> Multiple Pods on different nodes can mount the volume, but only in read-only mode.

ğŸ“¦ Example Use Case:
A shared config or dataset
A documentation website where everyone reads the same files

âœ… 3. ReadWriteMany (RWX) -> Multiple Pods on any number of nodes can read and write to the volume at the same time.

ğŸ“¦ Example Use Case:
Shared storage for logs or uploads
Web servers sharing a common upload directory

ğŸ”„ Behavior:

Pods across multiple nodes can all read and write to the volume at once
Most flexible, but not all cloud providers support it directly

ğŸ“Œ Note: On AWS, Azure, GCP:

RWX often needs NFS, Azure Files, Amazon EFS, or special provisioners.
EBS, AzureDisk, GCE PD support only RWO or ROX.

---------------------------------------------------------------------------------------------------------------------------------------------------

Persistent volume -> Persistent Volume (PV) is a storage resource in the cluster that exists independently of a Pod's lifecycle.

ğŸ”„ High-Level Flow

\* Admin creates a PV or a dynamic provisioning setup.
\* User creates a PersistentVolumeClaim (PVC) asking for storage.
\* Kubernetes binds the PVC to a matching PV.
\* A Pod mounts the PVC to use the storage.


ğŸ§­ PV Lifecycle Stages
| Stage         | What it Means                                                    |
| ------------- | ---------------------------------------------------------------- |
| **Available** | PV is created and ready to be claimed (not in use)               |
| **Bound**     | PV is bound (connected) to a PVC and in active use               |
| **Released**  | PVC is deleted, but PV is not yet cleaned up (still holds data)  |
| **Failed**    | Kubernetes tried to reclaim the PV (based on policy), but failed |



ğŸŒ€ Lifecycle of a PV and PVC

Stage Description
Available PV is created and unbound
Bound PV is bound to a PVC
Released PVC is deleted, but PV is not yet reclaimed
Failed PV can't be reclaimed automatically

ğŸ”„ Reclaim Policies

When a PVC is deleted, what happens to the PV?
Retain â†’ Keep the data (manual cleanup needed)
Recycle â†’ Simple cleanup (deprecated)
Delete â†’ Deletes the volume (used with cloud disks)
Retain -> "âš ï¸ When the PVC is deleted, do not delete the PersistentVolume (PV) or the actual storage/disk. Just mark it as Released and keep the data exactly as it is."

ğŸ§  Use Case for Retain:
You want to keep your data safe, even if the app or PVC is deleted.

Common for:
Databases (PostgreSQL, MySQL)
Forensics
Backups
Manually reusing disks

ğŸ§ª What happens when Retain is used?

âœ… You create a PV with persistentVolumeReclaimPolicy: Retain
âœ… You create a matching PVC
âœ… The PV and PVC are Bound
âŒ You delete the PVC
âš ï¸ PV status becomes Released
ğŸ“¦ Data still remains on the disk
ğŸ§¹ You need to manually do cleanup:

Delete the PV if no longer needed

Or manually bind it to another PVC (only possible if you clear its claimRef)

-------------------------------------------------------------------------------------------------------------------------------------------------

Configuration Data Storage:

a. ConfigMaps
b. Secrets
c. downwardAPI

ğŸ”· Why Configuration Storage?

Apps need config values like:
App settings (ENV=prod)
DB connection strings
Secret tokens
Pod metadata (pod name, IP, etc.)
You donâ€™t want to:
Hardcode them into your image
Rebuild Docker image on every change


ğŸ”§ Kubernetes provides 3 main ways:

| Type          | Stores                            | Scope                                         |
| ------------- | --------------------------------- | --------------------------------------------- |
| `ConfigMap`   | Non-sensitive config (plain text) | External app settings                         |
| `Secret`      | Sensitive data (base64-encoded)   | Passwords, tokens                             |
| `DownwardAPI` | Pod & container metadata          | Inject runtime data like Pod name, IP, labels |


ğŸŸ© A. ConfigMaps â€” App Configuration

A ConfigMap is used to store key-value pairs of non-sensitive config, such as:
App mode
Environment (dev, prod)
Feature flags
External URLs

ğŸ§¾ Example: Create a ConfigMap:

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_MODE: production
  FEATURE_FLAG: "true"


ğŸ“¦ Use in Pod:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo
spec:
  containers:
    - name: myapp
      image: busybox
      command: [ "sh", "-c", "echo Mode: $APP_MODE, Flag: $FEATURE_FLAG; sleep 3600" ]
      env:
        - name: APP_MODE
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: APP_MODE
        - name: FEATURE_FLAG
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: FEATURE_FLAG

â¡ï¸ Environment variables will be injected from the ConfigMap!



ğŸŸ¥ B. Secrets â€” Sensitive Config (Passwords, Tokens)

âœ… What is it?
A Secret is used to store sensitive data (in base64-encoded format), like:
Passwords
API keys
TLS certificates

Think of it like a ConfigMap, but for secrets with better security rules.

ğŸ§¾ Example: Create Secret (base64-encoded)

echo -n "admin" | base64 # => YWRtaW4=
echo -n "s3cr3t" | base64 # => czNjcjN0


apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: YWRtaW4=
  password: czNjcjN0


ğŸ“¦ Use in Pod:

env:
  - name: DB_USER
    valueFrom:
      secretKeyRef:
        name: db-secret
        key: username
  - name: DB_PASS
    valueFrom:
      secretKeyRef:
        name: db-secret
        key: password


âœ… Security:

Secrets are base64-encoded (not encrypted by default unless you enable encryption at rest)
Accessible only to the Pod that needs them
Can be mounted as files too

ğŸŸ¨ C. DownwardAPI â€” Inject Pod Metadata

Use DownwardAPI when you want to give your container information about itself, like:
Pod name
Pod IP
Namespace
Labels
CPU/memory limits

ğŸ§¾ Example: Inject Pod name and namespace

apiVersion: v1
kind: Pod
metadata:
  name: downwardapi-demo
  labels:
    env: test
spec:
  containers:
    - name: metadata-reader
      image: busybox
      command: [ "sh", "-c", "echo I am $MY_POD_NAME in $MY_NAMESPACE; sleep 3600" ]
      env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace


âœ… Real-World Use Cases

| Task                                 | Tool                      |
| ------------------------------------ | ------------------------- |
| Store app config for frontend        | `ConfigMap`               |
| Inject DB credentials securely       | `Secret`                  |
| Show Pod name in logs                | `DownwardAPI`             |
| Mount a config file in `/etc/config` | `ConfigMap` as volume     |
| Share API token with a backend       | `Secret` as env or volume |


ğŸ§  Summary Table
| Feature     | ConfigMap                | Secret                   | DownwardAPI                   |
| ----------- | ------------------------ | ------------------------ | ----------------------------- |
| Stores      | Non-sensitive config     | Sensitive data (base64)  | Pod/container metadata        |
| Usage Style | Key-value, env or volume | Key-value, env or volume | Inject env or file            |
| Security    | Open (readable by all)   | Access-controlled        | Built-in                      |
| Mountable?  | âœ… Yes                    | âœ… Yes                    | âœ… Yes                         |
| Common Use  | Feature flags, app mode  | DB passwords, tokens     | Pod name, IP, resource limits |


--------------------------------------------------------------------------------------------------------------------------------------------------------

ğŸš€ Deployment is wrapper of the pod. Deployment is superset of the pod. So within a deployment, you can either run a single pod or multiple pod.

\*> Why can't we directly execute the pod, which is executing my application?

--> And the answer is deployment is providing more feature and more flexibility, which is not supported with the pod using the deployment, you are getting a few more features.
-->A Deployment in Kubernetes is a controller that manages replica sets and Pods
What are those features? (scaling is the feature)

core feature{
scaling,
upgrade,
rollback
}

ğŸ”§ Key Responsibilities of a Deployment

| Feature                            | Description                                           |
| ---------------------------------- | ----------------------------------------------------- |
| ğŸ“¦ **Creates Pods**                | Automatically creates Pods using a template           |
| ğŸ” **Manages replicas**            | Ensures a specified number of Pods are always running |
| ğŸ”„ **Handles updates**             | Updates Pods to a new version without downtime        |
| ğŸ§  **Self-healing**                | Replaces failed or crashed Pods                       |
| ğŸ“š **Keeps history**               | Stores revision history for easy rollback             |
| ğŸ§ª **Supports rollout strategies** | Rolling updates, recreate strategies                  |



--> So one of them is killing.

Example: Suppose you are working for some kind of e-commerce platform, right? That e-commerce platform is expected to get more loads on the sale seasons, right? And when the sale is going to end, you are expecting the lower load.

So whenever your e-commerce platform is going to start the sale, you need more number of server, you need more number of resources to take care of the traffic, whatever the traffic you are getting in that particular sales season.

\*using deployment it will automatically scale/ descale your application (with help of replicas you can do that)

Deployment Strategy:

i. Recreate

What it is:
In this strategy, the old version of the application is shut down completely, and only then the new version is deployed.

Use Case:
Suitable for applications that donâ€™t require high availability.

Pros:
Simple to configure.
No version conflicts.

Cons:
Causes downtime during deployment.

ii. Rolling Upgrade

What it is:
This strategy updates Pods incrementally, replacing old Pods with new ones one by one, without downtime.

Use Case:
Default strategy in Kubernetes.
Good for high availability applications.

Example:
You have 10 replicas of v1.
Kubernetes slowly terminates one v1 Pod and starts one v2 Pod.
This continues until all Pods are v2.

Pros:
No downtime.
Easy to roll back.

Cons:
If a bug is introduced, users might still experience issues during rollout.


iii. Canary

What it is:
A small percentage of traffic is routed to the new version to test it in production before full rollout.

Use Case:
Useful for detecting bugs early with minimal impact.

Example:
Deploy 1 replica of v2 while keeping 9 replicas of v1.
Monitor the behavior.
If all looks good, gradually shift more traffic to v2.
Typically managed using:
Kubernetes with service mesh (like Istio, Linkerd) or tools like Flagger, Argo Rollouts.

Pros:
Safer rollouts.
Easy to monitor and abort if issues occur.

Cons:
Requires more complex routing logic.
Takes longer to complete.


iv. Blue Green Deployment

What it is:
Two environments are maintained:
Blue = current live version
Green = new version

After testing Green, you switch traffic to it instantly.
Use Case:
When you need zero downtime and instant rollback.

Example:
Blue (v1) is live.
Deploy Green (v2) in parallel.
Once verified, change the Service to point to Green.
If needed, roll back by pointing back to Blue.

Can be done using:
Separate namespaces or environments.
Tools like Argo Rollouts, Spinnaker, or manual service switch.

Pros:
Instant switch \& rollback.
Production testing possible.

Cons:
Double resource usage.
Complex to manage environments.

âœ… Key Differences Summary Table

| Strategy | Downtime | Rollback Ease | Complexity | Resource Usage | Best For |

| -------------- | -------- | ------------- | ---------- | -------------- | ----------------------------------- |

| Recreate | âœ… Yes | âŒ Difficult | âœ… Simple | âœ… Low | Simple apps, dev/test |

| Rolling Update | âŒ No | âœ… Easy | âœ… Moderate | âœ… Efficient | Most standard apps |

| Canary | âŒ No | âœ… Easy | âŒ Complex | âœ… Efficient | Risk-sensitive releases |

| Blue-Green | âŒ No | âœ… Instant | âŒ Complex | âŒ High | Critical apps needing zero-downtime |

---

ğŸ·ï¸ What Are Labels in Kubernetes?
Labels are key-value pairs attached to Kubernetes objects (like Pods, Deployments, Services, etc.).
They are used to identify, group, and select resources.

ğŸ“Œ Why Are Labels Important?
Group related resources together
Filter/select specific objects
Match objects to services, selectors, or deployments
Manage complex environments (e.g., dev, staging, prod)

ğŸ§  When to Use Labels?
To tag environments (env: dev, env: prod)
To group multiple Pods for a Deployment or Service
To identify owners, versions, app types
When you want to do rolling updates only on certain Pods

âœ… Example 1: Label in a Pod

apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
    app: webapp
    env: dev
spec:
  containers:
    - name: nginx
      image: nginx


ğŸ” Example 2: Selecting Pods using a Label

Imagine you have many Pods, but want a Service to route only to Pods with label app=webapp.

apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  selector:
    app: webapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80


The service will automatically find all Pods with app=webapp label and load balance between them.
âš ï¸ If you mismatch labels here, Service wonâ€™t manage those Pods.

ğŸ§ª Working With Labels in Commands
kubectl get pod --show-labels \\\\ to see all pod with label
kubectl get pods -l app=webapp \\\\ List Pods with a label:
kubectl label pod mypod tier=frontend \\\\ Add label to existing Pod:
kubectl label pod mypod tier- \\\\ Remove a label:
kubectl get pods -l "app=webapp,env=dev" \\\\ Filter by multiple labels:

ğŸ” What Are Selectors in Kubernetes?
Selectors are how Kubernetes matches or filters resources using labels.
They are used to select a group of resources (like Pods) based on the labels they carry.

ğŸ§  Why Are Selectors Used?

You use selectors to:
Connect a Service to the right Pods
Link a Deployment to its ReplicaSet and Pods
Target specific objects with kubectl
Group or filter resources logically

ğŸ“‚ Types of Selectors:

Equality-Based Selectors
Set-Based Selectors

âœ… 1. Equality-Based Selectors

These use:
= (equals)
== (equals)
!= (not equal)

eg:
selector:
  matchLabels:
    app: myapp
    tier: frontend


This will match any resource with:
app=myapp
AND tier=frontend

Command-line:
kubectl get pods -l app=myapp
kubectl get pods -l 'app=myapp,tier=frontend'

âœ… 2. Set-Based Selectors

These allow more powerful matching:
in: matches if label is in a list
notin: not in a list
exists: checks if a key exists

ğŸ“˜ Example:

selector:
  matchExpressions:
    - key: app
      operator: In
      values:
        - web
        - api
    - key: environment
      operator: NotIn
      values:
        - dev
    - key: tier
      operator: Exists


This means:
app must be web or api
environment must NOT be dev
tier must exist

ğŸ› ï¸ Practical Example: Service with Selector
Letâ€™s say you have a Pod with these labels:

metadata:
  labels:
    app: myapp
    role: backend


Now, create a Service that selects this Pod:

apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: myapp
    role: backend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080


ğŸ”„ Now Kubernetes will automatically connect this Service to any Pod matching those labels.

ğŸ§ª Practice: Use kubectl with Selectors

\# List Pods with a specific label
kubectl get pods -l app=myapp

\# List Pods where label "env" exists
kubectl get pods --selector='env'

\# List Pods where version is v1 or v2
kubectl get pods -l 'version in (v1,v2)'


ğŸ¯ Summary

| Selector Type    | Example                     | Purpose                  |
| ---------------- | --------------------------- | ------------------------ |
| matchLabels      | `app: frontend`             | Simple equality match    |
| matchExpressions | `key in [v1, v2]`           | Advanced logic           |
| CLI selector     | `-l app=myapp`              | Filtering with `kubectl` |
| Service selector | `spec.selector` in Service  | Connects to right Pods   |
| Deployment match | `spec.selector.matchLabels` | Links ReplicaSet to Pods |


when using selector with services we directly selector,
but when used with deployment , jobs we write

selector:
	matchlabels:

| Resource                | Selector Syntax                                    | Why?                                 |
| ----------------------- | -------------------------------------------------- | ------------------------------------ |
| **Service**             | `selector:` (simple key-value)                     | Simpler, just label match            |
| **Deployment**, **Job** | `selector:\n  matchLabels:` or `matchExpressions:` | More control & strict matching logic |


----------------------------------------------------------------------

**DEPLOYMENT STRATEGY**

--> ROLLING UPDATE IS DEFAULT STRATEGY OF DEPLOYMENT

ğŸŸ© What is the Recreate Strategy?
All the old Pods are stopped first, then the new Pods are created.

ğŸ“„ YAML Example: Recreate Deployment Strategy

apiVersion: apps/v1
kind: Deployment
metadata:
  name: recreate-demo
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web-container
          image: nginx:1.21  # later update to nginx:1.22


ğŸ”„ What happens when image is updated?

kubectl set image deployment/recreate-demo web-container=nginx:1.22

Kubernetes will:

1.Kill both old Pods (nginx:1.21)
2.Then start 2 new Pods (nginx:1.22)

ğŸ§ª How to Observe It

kubectl rollout status deployment recreate-demo
kubectl get pods -l app=web -w


âœ… Pros of Recreate Strategy
| Feature                  | Benefit                                                      |
| ------------------------ | ------------------------------------------------------------ |
| âŒ No overlap             | Good for apps that canâ€™t run multiple versions               |
| âœ… Simple                 | Easy to reason about                                         |
| ğŸ§  Safe in special cases | For apps with exclusive locks, databases, or migration logic |


âŒ Cons of Recreate Strategy
| Issue                               | Impact                                         |
| ----------------------------------- | ---------------------------------------------- |
| â›” Downtime                          | All Pods are stopped before new ones come up   |
| âŒ No high availability              | Traffic will fail during the switch            |
| ğŸš« Not good for production web apps | Bad for user-facing APIs that need 100% uptime |


----------------------------------------------------------------------------------------

**ğŸ”„ What is the RollingUpdate Strategy?**

In a RollingUpdate, old Pods are gradually replaced with new Pods â€” one (or a few) at a time.

This ensures:
Zero downtime
Continuous availability of the application

ğŸŒ€ How It Works:

Imagine you have 4 Pods running version 1 (v1), and you want to update to version 2 (v2):
Kubernetes starts 1 new Pod with version 2
Waits for it to be ready
Then deletes 1 old Pod
Repeats the process until all old Pods are replaced with new ones
At no point are all Pods down â€” your app keeps running.

**ğŸ“„ YAML Example: RollingUpdate Strategy**

apiVersion: apps/v1
kind: Deployment
metadata:
  name: rolling-update-demo
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1       # At most 1 Pod can be unavailable at a time
      maxSurge: 1             # At most 1 extra Pod can be created above desired count
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp-container
          image: nginx:1.21


ğŸ”„ Updating the Deployment

Letâ€™s say you want to update the image to nginx:1.22:
kubectl set image deployment/rolling-update-demo myapp-container=nginx:1.22

Now Kubernetes will:
Create 1 new Pod
Wait for it to be healthy
Delete 1 old Pod
Repeat until all Pods are updated

âš™ï¸ Important Parameters

| Field            | Meaning                                                                             |
| ---------------- | ----------------------------------------------------------------------------------- |
| `maxUnavailable` | Max number of Pods that can be **down** during the update (default is 25%)          |
| `maxSurge`       | Max number of **extra Pods** that can be created during the update (default is 25%) |


âœ… Good Practice Defaults:

maxUnavailable: 0 # Always keep all running
maxSurge: 1 # Add 1 extra pod at a time

ğŸ‘€ Monitor the Update

kubectl rollout status deployment rolling-update-demo
kubectl get pods -l app=myapp -w

âœ… Pros of RollingUpdate

| Advantage        | Description                                            |
| ---------------- | ------------------------------------------------------ |
| âœ… Zero downtime  | Your app remains available throughout                  |
| âœ… Safer          | If something goes wrong, only a few Pods are affected  |
| ğŸ” Can be paused | You can pause the rollout, fix the issue, and resume   |
| ğŸ”™ Easy rollback | Built-in rollback mechanism via `kubectl rollout undo` |



âŒ Cons of RollingUpdate

| Limitation                  | Description                                                   |
| --------------------------- | ------------------------------------------------------------- |
| âš ï¸ Slower                   | It takes time to update all Pods one by one                   |
| âŒ Complex for stateful apps | May not work well with shared volumes or DB migration apps    |
| ğŸ’° Resource spike           | Because of `maxSurge`, more resources may be used temporarily |


ğŸš¦ When to Use

âœ… Use RollingUpdate when:

You want zero downtime
Your app can handle multiple versions running temporarily
Your app is stateless (like web apps, APIs, etc.)

âŒ Avoid for:

Apps with stateful logic
Apps that canâ€™t run multiple versions at once â†’ use Recreate

ğŸ”„ Bonus: Rollback
kubectl rollout undo deployment rolling-update-demo

**_two main ways to manage or upgrade your applications_**

ğŸ§­ 1. Imperative Approach - > Directly updates the image of the container, No YAML file involved
kubectl set image deployment/myapp mycontainer=nginx:1.25

âœ… Use Imperative when:
You want quick fixes or experiments
Youâ€™re testing/debugging
You donâ€™t need version tracking

ğŸ“œ 2. Declarative Approach -> You declare the desired state of your application in a YAML file and apply it.
âœ… Use Declarative when:
You want repeatable deployments
Youâ€™re using Git for CI/CD or GitOps
Youâ€™re working in a team or production environment
You need to keep a history of your configuration

------------------------------------------------------

ğŸš€ What is Scaling in Kubernetes?
Scaling a Deployment means increasing or decreasing the number of replicas (pods) that are running for an application.

ğŸ›  How to Scale a Deployment

ğŸ”§ 1. Imperative (using kubectl)

âœ… Scale Up/Down manually:
kubectl scale deployment myapp --replicas=5
This command tells Kubernetes to run 5 pods for myapp Deployment.

for descale:: kubectl scale deployment myapp --replicas=0

ğŸ“„ 2. Declarative (via YAML)

Update the replicas field in your YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 3        # ğŸ‘ˆ Change this value
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: mycontainer
        image: nginx


ğŸ“ˆ Auto-scaling

kubectl autoscale deployment myapp --cpu-percent=50 --min=2 --max=10

ğŸ§  Recap

| Action       | Method                     |
| ------------ | -------------------------- |
| Manual scale | `kubectl scale`            |
| Declarative  | Change `replicas:` in YAML |
| Descale      | Set `replicas=0`           |
| Auto-scale   | Use `kubectl autoscale`    |


----------------------------------------------------------------------------------------

ğŸ¦ What is Canary Deployment?

It was implemented by facebook firstly in 2011
Canary Deployment is a progressive rollout strategy where:
A small percentage of users (or traffic) is exposed to the new version first.
If no issues are found, gradually more users are shifted to the new version.
If a bug is found, you can quickly rollback without impacting all users.

ğŸ¯ When Should You Use It?

| Use Case                         | Canary Ideal?   |
| -------------------------------- | --------------- |
| Testing a **new feature**        | âœ… Yes           |
| Reducing **risk of failure**     | âœ… Yes           |
| Gathering **real-user feedback** | âœ… Yes           |
| Highly **critical systems**      | âœ… Yes           |
| Small team / non-production      | âŒ Not necessary |


âš™ How Does It Work in Kubernetes?

In Kubernetes, Deployments donâ€™t support canary natively â€” but you can do it in two ways

ğŸ”§ Option 1: Manual Canary with Two Deployments
Create two separate Deployments:
myapp-v1 (stable version)
myapp-v2 (new version)
Then expose both behind a Service using labels and control how much traffic goes to each.

ğŸ“„ Example

**Stable Deployment**

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v1
spec:
  replicas: 4
  selector:
    matchLabels:
      app: myapp
      version: v1
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: app
        image: myapp:v1

**Canary Deployment**

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v2
spec:
  replicas: 1   # ğŸ‘ˆ Canary!
  selector:
    matchLabels:
      app: myapp
      version: v2
  template:
    metadata:
      labels:
        app: myapp
        version: v2
    spec:
      containers:
      - name: app
        image: myapp:v2

Service
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
spec:
  selector:
    app: myapp   # ğŸ‘ˆ Matches both v1 and v2
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080


â¡ï¸ This setup sends 80% traffic to v1, 20% to v2 (based on pod counts).
You can increase replicas of v2 slowly, and decrease v1 â€” until you're fully on v2.

**ğŸ”§ Option 2: Advanced Canary via Service Mesh (Istio, Linkerd)**
Service meshes allow you to split traffic by percentage (e.g., 90% to v1, 10% to v2) regardless of replica counts.

âœ… More precise
âŒ More complex setup

âœ… Pros of Canary Deployment
âœ… Safer than full deployment
âœ… Easier to rollback if something breaks
âœ… Real-user feedback before full release
âœ… Great for A/B testing

âŒ Cons of Canary Deployment
âŒ More complex to set up than Rolling Update
âŒ Requires careful monitoring
âŒ May need traffic routing tools (e.g., service mesh)
âŒ Logs/tracing need to be split between versions

ğŸš€ Summary

| Tip                         | Why                      |
| --------------------------- | ------------------------ |
| Start with small percentage | Limit risk               |
| Monitor logs and metrics    | Catch issues early       |
| Automate rollout/rollback   | Reduce manual work       |
| Use labels properly         | Clean version separation |
| Document the process        | Avoid confusion          |


------------------------------------------------------------------------------------------------------------------------------

**ğŸ’¡ What is Blue-Green Deployment?**

Blue-Green Deployment is a release technique where you run two environments (old and new) side-by-side:
Blue = current/old version of the app.
Green = new version you want to deploy.
At deployment time, traffic is switched from Blue to Green â€” instantly and safely.

âœ… Key Idea:
Instead of upgrading the existing version, you deploy a new version alongside, and then switch traffic using a Service

ğŸ¯ When to Use It?
You want zero downtime deployments.
You need a safe rollback path (go back to Blue if Green fails).
Your application state can be easily duplicated.

ğŸ”§ How it works in Kubernetes?
Create two deployments:
myapp-blue (v1)
myapp-green (v2)
Use a Service that selects one of them based on labels.

**ğŸ“„ Example: Blue-Green YAML**

myapp-blue.yaml â€“ Old Version

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: myapp
        image: nginx:1.21


myapp-green.yaml â€“ New Version

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-green
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: myapp
        image: nginx:1.25


service.yaml â€“ Switch Target

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
    version: blue   # ğŸ‘ˆ Switch this to 'green' when ready!
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80


ğŸ”„ To Promote Green:
spec:
  selector:
    app: myapp
    version: green



This instantly routes traffic to the new version without restarting pods.

ğŸŸ¢ Pros
âœ… Zero-downtime deployment
âœ… Easy and instant rollback
âœ… Can run health checks on new version before going live
âœ… Clear separation between old and new

ğŸ”´ Cons
ğŸš« Requires double the resources (CPU, memory, etc.)
ğŸš« Can be complex for stateful applications (e.g., databases)
ğŸš« Needs careful service switch and cleanup logic
ğŸš« Manual or scripted service update unless using tools like ArgoCD, Spinnaker

**Rollback?**

**Just change the service back to Blue:**

| Use Case                      | Blue-Green Fit? |
| ----------------------------- | --------------- |
| Stateless microservices       | âœ… Perfect       |
| Web frontend apps             | âœ… Ideal         |
| Large, stateful DB migrations | âŒ Risky         |
| Apps needing zero downtime    | âœ… Recommended   |


ğŸ“Œ Summary

| Feature        | Blue-Green Deployment   |
| -------------- | ----------------------- |
| Downtime       | Zero (instant switch)   |
| Rollback       | Instant                 |
| Risk           | Low if automated        |
| Resource usage | High (two environments) |
| Complexity     | Medium                  |


in a Blue-Green deployment, when you update the Service selector from Blue to Green in Kubernetes, the switch is virtually instant (in milliseconds to a few seconds), and it does not cause downtime if everything is configured properly.

Kubernetes updates the Endpoints behind the Service. This is done by the Kube-proxy, and it:
Stops sending traffic to Blue pods
Starts sending traffic to Green pods
Does not restart any pod or reload configuration â€” just updates routing



BEFORE PUSHING GREEN TO THE MAIN SERVICE , FORWARD IT TO SOME PORT AND TEST IT.
 Test GREEN (Without changing service)
Use kubectl port-forward to access one Green Pod directly:

kubectl get pods -l version=green
# Pick a Green pod name, e.g., myapp-green-xxxxx

kubectl port-forward pod/myapp-green-xxxxx 9090:5678
# Then test in another terminal:
curl http://localhost:9090
# Should return: Hello from GREEN

For DB Migration, you can use DB Migration Tool:
Liquibase
Flyway
Alembic (Python)
Rails Migrations
Prisma Migrate (Node.js)

------------------------------------------------------------------------------------------------------

### **ğŸš€ What is Helm?**

Helm is like apt (Debian) or yum (RedHat) â€” but for Kubernetes applications.
âœ… It helps you define, install, upgrade, and manage Kubernetes applications using reusable templates and charts.

###### ğŸ“¦ Why Use Helm?

Without Helm:
You manage dozens of YAML files for each app: deployments, services, PVCs, etc.
Reusability is low.
Hard to version and manage configs.

With Helm:
One command deploys your whole app stack.
Configuration is centralized and clean.
Easily install/update/delete apps.

###### ğŸ”§ Key Concepts in Helm

| Concept          | Description                                                                                                             |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Chart**        | A Helm package. Contains YAML templates, default values, and metadata. Think of it like a folder that defines your app. |
| **Values**       | Configuration settings (in `values.yaml`) that are passed into your templates.                                          |
| **Release**      | A running instance of a Helm chart in a Kubernetes cluster.                                                             |
| **Repositories** | Where charts are stored (like Docker Hub for Helm charts).                                                              |


##### ğŸ›  Helm Folder Structure

When you create a chart:

helm create myapp

It generates:

myapp/
â”œâ”€â”€ Chart.yaml         # Chart metadata
â”œâ”€â”€ values.yaml        # Default values
â”œâ”€â”€ templates/         # All k8s YAML templates go here
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ _helpers.tpl   # Template helpers


You write your Kubernetes YAML inside templates/, but with variables like:

apiVersion: v1
kind: Service
metadata:
  name: {{ .Values.service.name }}


And define the actual value in values.yaml:

service:
  name: my-app-service


###### âœ… Pros of Helm
ğŸš€ Fast Deployments: Deploy an entire app stack in one command.
ğŸ”„ Easy Upgrades/Rollbacks: Manage versioned deployments.
ğŸ“¦ Package Ecosystem: Install 3rd-party apps like MySQL, Prometheus, NGINX with 1 command.
ğŸ”§ Templating: Create dynamic Kubernetes manifests.

###### âŒ Cons of Helm

â— Complexity for very simple apps.
ğŸ§  Learning curve for templating syntax (Go templates).
ğŸ” Secrets need care (Helm doesn't encrypt them by default).
ğŸ§ª Debugging templating errors can be tricky.

##### ğŸ“¥ Deploying a 3rd-Party App using Helm

Example: Install MySQL

helm repo add bitnami https://charts.bitnami.com/bitnami
helm install my-mysql bitnami/mysql


\## This installs a MySQL

##### ğŸ“¦ Creating Your Own Helm Package

1. Create a chart:

helm create myapp

2\. Modify templates/ to add your deployment, service, etc
3\. Set defaults in values.yaml
4\. Install:

helm install myapp ./myapp --> it creates all the Kubernetes resources (like Deployment, Services, PVCs, etc.) , those resources will be created in your Kubernetes cluster

ğŸ” 2. Should you run helm lint before helm install?

helm lint ./myapp --->>> This will validate the syntax of your Helm chart, like Broken indentation, Missing values, invalid syntax.

If there are errors, fix them before running helm install.

ğŸ§ª Example workflow:

helm create myapp # Step 1: scaffold chart
helm lint ./myapp # Step 2: validate chart
helm install myapp ./myapp # Step 3: install to cluster
kubectl get all # Step 4: verify resources

ğŸ”„ Upgrading, Rolling Back, and Uninstalling

\# Upgrade app
helm upgrade myapp ./myapp --set replicaCount=3

\# Rollback to previous revision
helm rollback myapp 1

\# Uninstall
helm uninstall myapp

##### ğŸ“š Important Helm Commands

| Command                  | Description                                     |
| ------------------------ | ----------------------------------------------- |
| `helm repo add`          | Add a chart repo (like adding apt repo)         |
| `helm install`           | Install a chart                                 |
| `helm upgrade`           | Upgrade a release                               |
| `helm rollback`          | Rollback to an older release                    |
| `helm uninstall`         | Remove a deployed app                           |
| `helm list`              | List all installed releases                     |
| `helm template`          | Render the template without applying to cluster |
| `helm show values`       | Show default values for a chart                 |
| `helm get all <release>` | Show all info about a release                   |
| `helm lint`              | Check for errors in your chart                  |


###### ğŸ’¡ Real Use Case: Deploying Your App with Helm

Step 1: Create your Helm chart
helm create my-webapp

Step 2: Edit deployment.yaml like this:

metadata:
  name: {{ .Values.appName }}
spec:
  replicas: {{ .Values.replicaCount }}


Step 3: Set default values in values.yaml

appName: webapp
replicaCount: 2


Step 4: Deploy to Kubernetes
helm install webapp ./my-webapp


ğŸ”’ Managing Secrets in Helm

Helm doesn't encrypt secrets by default. Use:

SealedSecrets
helm-secrets plugin (uses SOPS)
External Secrets Operator

ğŸ¯ When to Use Helm?

âœ… Use Helm when:
You have complex applications with multiple Kubernetes resources.
You want to parameterize configs.
You want to install third-party tools.
You want to manage releases, upgrades, rollback cleanly


ğŸš€ Goal:

Weâ€™ll create a Helm chart that deploys a simple Nginx web server, then install it on your Kubernetes cluster.

âœ… Prerequisites:
Make sure you have these installed and configured:
helm (v3+)
kubectl
Access to a Kubernetes cluster (like Minikube, AKS, EKS, etc.)

ğŸ§± Step 1: Create a Helm chart

helm create my-nginx

This creates a folder structure like:

my-nginx/
â”œâ”€â”€ charts/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ values.yaml
â”œâ”€â”€ Chart.yaml


âœï¸ Step 2: Customize it for Nginx

Open values.yaml and update the image section:

image:
  repository: nginx
  pullPolicy: IfNotPresent
  tag: "latest"

This tells the chart to deploy an nginx container.


ğŸ“¦ Step 3: (Optional but Good) Lint the chart

helm lint ./my-nginx

ğŸš¢ Step 4: Install the Helm chart

helm install nginx-release ./my-nginx # This deploys your Nginx server to the cluster.

You can verify with:

kubectl get pods
kubectl get svc

ğŸ§ª Step 5: Test access

If you're using Minikube:
minikube service nginx-release

Or port-forward:
kubectl port-forward svc/nginx-release 8080:80

\#Now go to http://localhost:8080 in your browser â€” you should see the default Nginx welcome page ğŸ‰


\##Create HELM Chart

helm create

Name of the chart provided here will be the name of the directory where the chart is created and stored.
Let's understand the relevance of these files and folders created for us:

Chart.yaml: This is the main file that contains the description of our chart
values.yaml: this is the file that contains the default values for our chart
templates: This is the directory where Kubernetes resources are defined as templates
charts: This is an optional directory that may contain sub-charts

\##HELM Commands for Chart

helm lint #This is a simple command that takes the path to a chart and runs a battery of tests to ensure that the chart is well-formed
helm template #This will generate all templates with variables , for quick feedback, and show the output. Now that we know everything is OK, we can deploy the chart:
helm install #Run this command to install the chart into the Kubernetes cluster:
helm ls --all #We would like to see which charts are installed as what release.
helm upgrade #This command helps us to upgrade a release to a specified or current version of the chart or configuration:
helm rollback #This is the command to rollback a release to the previous version:
helm delete --purge #We can use this command to delete a release from Kubernetes:

--------------------------------------------------------------------------------------------------------------------


# Application Observability and Maintenance

###### ğŸ” What is a Liveness Probe?

A Liveness Probe checks whether your application (container) is alive â€” meaning it hasn't crashed or hung.
If the liveness probe fails repeatedly, Kubernetes kills the container and restarts it automatically.
This helps to self-heal apps that are stuck or in bad state.

ğŸ§  Why Use Liveness Probes?

Without probes:
If your app crashes silently or gets stuck, Kubernetes wonâ€™t know.
It may keep the container running forever in a broken state.

With liveness probes:
Kubernetes monitors it and automatically restarts unhealthy containers.

âœ… Types of Probes

Kubernetes supports 3 types of probes:

| Probe Type  | What it Does                                          |
| ----------- | ----------------------------------------------------- |
| `httpGet`   | Sends HTTP GET request to container (e.g., `/health`) |
| `tcpSocket` | Opens TCP connection on a port                        |
| `exec`      | Runs a shell command inside the container             |


ğŸ’¡ Liveness Probe Syntax Example (YAML)

livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 3
  timeoutSeconds: 2


ğŸ” Explanation:

httpGet.path = API path that should return 200 OK

port = Port app listens on

initialDelaySeconds = Wait before first probe (give app time to boot), Number of seconds after the container has started before liveness or readiness probes are initiated.

periodSeconds = Time between checks, How often to perform the probe (in seconds)

failureThreshold = How many failures before Kubernetes restarts , Min consecutive failure for the probe before giving up or restarting.

timeoutSeconds = Timeout for each check , Number of seconds after which the probe times out (eg: if that http call is not responding like api then it will mark it as failed)

successThreshold = Min Consecutive success for the probe to be considered successful after having failed.

ğŸ§ª Full Working Example : HTTP METHOD

apiVersion: v1
kind: Pod
metadata:
  name: nginx-liveness
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 10


This sends a GET request to / every 10 seconds after an initial delay of 5 seconds.
If Nginx stops responding (e.g., port blocked, service crashed), Kubernetes restarts it.

ğŸ§¾ Full YAML (Liveness Probe with exec)

apiVersion: v1
kind: Pod
metadata:
  name: liveness-exec-pod
spec:
  containers:
  - name: liveness-demo
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      touch /tmp/healthy;
      sleep 30;
      rm -f /tmp/healthy;
      sleep 3600;
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 1


ğŸ” What this does:

The container:

Creates a file /tmp/healthy
Waits 30 seconds
Deletes the file (to simulate app failure)
Then keeps sleeping

The liveness probe:
Starts checking after 5 seconds
Every 5 seconds, it runs cat /tmp/healthy
After 30 seconds, the file is deleted â†’ cat fails â†’ probe fails â†’ Kubernetes restarts the container

âœ… How to Test

Apply it:
kubectl apply -f liveness-exec.yaml

Describe the Pod:
kubectl describe pod liveness-exec-pod

Watch for Liveness probe failed events, and you'll see restarts increasing:
kubectl get pod liveness-exec-pod

###### âœ… Full YAML â€“ Liveness Probe with tcpSocket

apiVersion: v1
kind: Pod
metadata:
  name: liveness-tcp-pod
spec:
  containers:
  - name: mysql-simulator
    image: busybox
    args:
    - /bin/sh
    - -c
    - |
      while true; do
        nc -lk -p 3306 -e echo "OK";
      done
    ports:
    - containerPort: 3306
    livenessProbe:
      tcpSocket:
        port: 3306
      initialDelaySeconds: 5
      periodSeconds: 10
      failureThreshold: 2


ğŸ” Explanation

Container simulates a MySQL service by listening on TCP port 3306 using netcat (nc).
livenessProbe.tcpSocket:
Starts checking after 5 seconds.
Every 10 seconds, Kubernetes checks if port 3306 is open.
If 2 consecutive failures, container is restarted.

ğŸ” Liveness Probe Types â€“ Full Comparison

| Type        | How it Works                            | Use Case                                                        | Example Command                     |
| ----------- | --------------------------------------- | --------------------------------------------------------------- | ----------------------------------- |
| `exec`      | Runs a command **inside the container** | When app has a CLI or command to check health                   | `cat /tmp/healthy`                  |
| `httpGet`   | Makes an **HTTP GET request**           | When app exposes a **health check endpoint** (e.g., `/healthz`) | `curl http://localhost:8080/health` |
| `tcpSocket` | Tries to **open a TCP connection**      | For basic service (like DBs) to check if port is open           | Connects to `localhost:<port>`      |


ğŸ”§ 1. exec Probe

âœ… Use When:

You have a custom logic that can be checked using a script or CLI.
No HTTP endpoint is available.
Lightweight and quick checks.

âŒ Avoid When:

Your check logic is complex or may hang.
The image doesn't support shell or the probe needs multiple tools.

ğŸŒ 2. httpGet Probe

âœ… Use When:

Your app exposes a HTTP health check endpoint, e.g., /healthz.
Ideal for web apps, REST APIs, microservices.

âŒ Avoid When:

No HTTP server or health check route.
Authentication or SSL complicates the check.

ğŸ”Œ 3. tcpSocket Probe

âœ… Use When:

You want to check if a port is open and accepting connections.
Useful for databases, queue systems, etc., where HTTP isn't exposed.

âŒ Avoid When:

Port open â‰  app is healthy (it could hang inside).
Doesnâ€™t check business logic or app status.

ğŸ§  Summary: When to Use What?

| Scenario                            	  | Recommended Probe |
| ----------------------------------- | ----------------- |
| Simple file-based or CLI check      | `exec`            |
| Web service with HTTP health checks | `httpGet`         |
| Database or port-level check        | `tcpSocket`       |

---------------------------------------------

#### ğŸŸ¢ What is a Readiness Probe?

Readiness gets executed before liveness.
A Readiness Probe tells Kubernetes whether your container is ready to accept traffic.
Even if a pod is running, it may not be ready to serve requests (e.g., still loading data, initializing, etc). This is where readiness probes come in.
we can say it runs like initial health check
Readiness Probe is inital probe which will execute when container started whereas liveness probe is a continous probe which will continue checking application and as soon as app start responding it will restart

ğŸ” Why is it important?

When a container fails the readiness probe, Kubernetes removes it from the Service endpoints, so it stops receiving traffic.
It is NOT restarted (unlike with Liveness probe).
Once the probe passes again, traffic is resumed.

âœ… Example Use Cases

App needs time to warm up
Loading models into memory
Waiting for DB connection
Waiting for cache to be populated

ğŸ§ª Types of Readiness Probes

Just like liveness probes, three types:

exec â€” Run a command in container
httpGet â€” Perform HTTP GET
tcpSocket â€” Try TCP connection

ğŸ“˜ Example YAML â€“ Readiness Probe

apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
spec:
  containers:
  - name: myapp
    image: myapp:1.0
    ports:
    - containerPort: 8080
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20

ğŸ” Key Differences: Liveness vs Readiness

| Feature       	 	    | **Liveness Probe**              		   | **Readiness Probe**                            |
| ----------------------| ------------------------------------------ | ---------------------------------------------- |
| Purpose        		    | Checks if container is **alive**  | Checks if container is **ready** to serve      |
| Failing Action     | Container is **restarted**      	   | Removed from **Service endpoints**, no traffic |
| Traffic Control 	| No impact                        				  | Stops traffic to container                     |
| Restart Trigger 	| Yes                           					     | No                                             |
| Use Case       			 | App stuck, deadlock, crash     	   | Initialization delay, temporary unavailability |
| Impact on Pod  	    | Restarted by Kubelet           		  | Just marked **Unready**                        |

ğŸ§  Summary

Use Readiness to control traffic.
Use Liveness to auto-heal dead containers.

They serve different roles and are often used together.
â“1. Will a Readiness Probe Restart the Pod If It Fails?
ğŸ”´ NO, a readiness probe will not restart the pod if it fails.
When a readiness probe fails, Kubernetes simply removes the pod from the list of endpoints in the Service â€” it stops sending traffic to it.
The pod keeps running normally.
Once the readiness probe starts passing again, the pod is added back to the endpoints list.

ğŸ“Œ Use Case: Think of it like a "pause" on traffic â€” the app may be doing some maintenance or temporarily unavailable.

â“2. Will a Liveness Probe Be Blocked If Readiness Probe Fails?
ğŸ”µ NO, the liveness probe works independently of the readiness probe.
Both probes are configured and executed separately by the kubelet.
Even if readiness fails, liveness will still be executed on schedule.
So a container can be unready but alive â€” and vice versa.

â“3. If Readiness Fails, Is the Pod Marked as Unready?
âœ… YES, the pod is marked as NotReady.
ğŸ”„ Key Fact:
Liveness and Readiness probes are completely independent.
Even if readiness probe fails, the liveness probe still runs.

ğŸ§  Why It Might Seem Like Readiness Controls Liveness

You're probably thinking:
â€œIf the pod is not ready (due to readiness failure), it shouldn't be checked for liveness, right?â€

But in reality:
Kubernetes separates "Is this pod healthy?" (liveness) from "Is this pod ready to serve traffic?" (readiness).
Liveness probes help the Kubelet decide whether to restart the container.
Readiness probes help the Kube Proxy/Service layer decide whether to send traffic to the container.

âœ… What Actually Happens

Letâ€™s assume a pod with both probes configured like this:

livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5


Even if /ready fails every time (readiness fails), the kubelet still continues to send /healthz requests for the liveness probe.

This is by design:
Kubernetes wants to ensure that a container that is "not ready" isn't also stuck.
If liveness fails, it restarts the container.
If readiness fails but liveness passes, the container stays running but is not given traffic.

ğŸ§ª Demo Scenario to Prove It (Thought Experiment)

Letâ€™s say:
App is stuck in a boot loop â€” never gets ready (/ready always fails).
But app is not crashing â€” /healthz still returns 200.

Result:
Readiness probe fails â†’ Pod is marked NotReady â†’ No traffic.
Liveness probe passes â†’ Pod is not restarted.
Kubernetes just patiently waits until the readiness probe starts passing

---------------------------------------------------------------------------------------------------------------------------------
